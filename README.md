#ğŸ¤– Multilingual Emotion & Sarcasm Detection


##ğŸ“– Abstract

Understanding human communication requires more than just literal text analysis. Emotions and sarcasm are crucial components of natural language but are challenging for machines to interpret, especially in multilingual settings.

This project presents a Streamlit-based interactive system that integrates fine-tuned transformer models (XLM-RoBERTa) for multilingual emotion classification and sarcasm detection. The system supports real-time inference, as well as bulk analysis of WhatsApp/Telegram chats, with results visualized through analytics dashboards.

##ğŸ§© Motivation

Ambiguity in Language

Example: â€œWow, great job ğŸ‘â€ can either be genuine praise or sarcasm depending on context.

Social Media Complexity

Platforms like WhatsApp, Twitter, and Telegram contain highly informal and emotionally nuanced text.

Multilingual Challenge

Existing emotion/sarcasm systems are often restricted to English.

Our approach leverages XLM-RoBERTa for cross-lingual generalization.

##ğŸ¯ Objectives

Build a robust emotion classifier capable of detecting six emotions: Joy, Love, Anger, Fear, Sadness, Surprise.

Develop a sarcasm detection model trained on irony datasets.

Provide a user-friendly interface for both single-message predictions and chat-level analytics.

Demonstrate the effectiveness of transfer learning using pre-trained multilingual transformers.

##ğŸ“š Related Work

Emotion Detection

Traditional methods (SVMs, NaÃ¯ve Bayes) are weak in multilingual settings.

Transformers like BERT and XLM-RoBERTa achieve state-of-the-art results.

Sarcasm Detection

Prior research highlights sarcasm as context-heavy, often requiring multimodal signals.

This project focuses on text-based sarcasm detection using TweetEval - Irony Dataset.

##Applications

Sentiment analysis

Mental health monitoring

Conversational AI

Social media moderation


                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  User Input / Chat File  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Preprocessing    â”‚
                      â”‚ (Tokenizer, PAD) â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Fine-tuned XLM-RoBERTa     â”‚
                â”‚ Emotion Model (6 classes)  â”‚
                â”‚ Sarcasm Model (binary)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Predictions (Emotion+Irony)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Streamlit Dashboard (UI + Graphs) â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


##âš™ï¸ Methodology
1. Datasets

Emotion Classification â†’ HuggingFace Emotion Dataset
 (6-class).

Sarcasm Detection â†’ TweetEval - Irony Dataset
 (binary).

##2. Model Architecture

Base Model: xlm-roberta-base

Fine-tuning Strategy:

Max sequence length: 128

Batch size: 64

Optimizer: AdamW

Learning rate: 2e-5

Epochs: 2â€“3

##3. Training Pipeline

Tokenization with XLMRobertaTokenizer

Data batching & shuffling

CrossEntropy loss optimization

Evaluation via Accuracy & Weighted F1

##4. Deployment

Frontend: Streamlit UI

Backend: Fine-tuned XLM-R models

##Features:

Real-time inference

Chat file uploader

Analytics dashboards

##ğŸ”¬ Experiments & Results
ğŸ“Œ Emotion Model (6-class)
Metric	Value
Validation Accuracy	~90%
Weighted F1 Score	~0.88
##ğŸ“Œ Sarcasm Model (Binary)
Metric	Value
Validation Accuracy	~85%
F1 Score	~0.82

ğŸ“Š Training curves:
<<img width="1200" height="400" alt="training_metrics" src="https://github.com/user-attachments/assets/20d5c6e1-e334-4742-8105-3cb85043ffeb" />
 />



##ğŸ“Š Analytics

Emotion Distribution â†’ Bar charts of detected emotions.

Sarcasm Distribution â†’ Ratio of sarcastic vs non-sarcastic text.

Chat Insights â†’ Long-term sentiment/sarcasm trends in conversations.

##ğŸ“Œ Limitations

Text-only sarcasm detection (no tone/emoji/video context).

Training requires GPU resources.

Fine-tuned mostly on Twitter data (domain-specific).

##ğŸš§ Future Work

âœ… Incorporate multimodal sarcasm detection (emoji, audio, video cues).

âœ… Add context-aware conversation modeling.

âœ… Deploy on HuggingFace Spaces / Streamlit Cloud.

âœ… Integrate with live messaging apps.

##ğŸ§‘â€ğŸ’» Tech Stack

Language: Python 3.10+

Framework: Streamlit

Models: HuggingFace Transformers (XLM-RoBERTa)

Visualization: Matplotlib, Pandas

#ğŸ“š References

Conneau, A., et al. (2020). Unsupervised Cross-lingual Representation Learning at Scale. ACL.

Mohammad, S., et al. (2018). SemEval-2018 Task 1: Affect in Tweets.

Barbieri, F., et al. (2020). TweetEval: Unified Benchmark for Tweet Classification.
